"""
AI Bookmark Classifier Core
AI书签分类器核心模块

集成了多种AI技术的智能书签分类器：
- 基于规则的快速匹配
- 机器学习模型预测
- 深度学习语义理解
- 用户行为学习
"""

import os
import json
import logging
from typing import Dict, List, Tuple, Optional
from dataclasses import dataclass, field
from datetime import datetime
from collections import defaultdict
import hashlib
import re
from urllib.parse import urlparse

# 导入子模块
try:
    from .ml_classifier import MLClassifierWrapper
except ImportError:
    MLClassifierWrapper = None

# LLM 分类器
try:
    from .llm_classifier import LLMClassifier
except ImportError:
    LLMClassifier = None

from .rule_engine import RuleEngine

# 导入智能规则加载器
try:
    from .smart_rule_loader import SmartRuleLoader, merge_with_main_config
except ImportError:
    SmartRuleLoader = None
    merge_with_main_config = None

# 导入占位符模块
from .placeholder_modules import (
    SemanticAnalyzer, UserProfiler, PerformanceMonitor
)


@dataclass
class BookmarkFeatures:
    """书签特征"""
    url: str
    title: str
    domain: str
    path_segments: List[str]
    query_params: Dict[str, str]
    content_type: str
    language: str
    timestamp: datetime = field(default_factory=datetime.now)

    @property
    def url_length(self) -> int:
        return len(self.url)

    @property
    def title_length(self) -> int:
        return len(self.title)

    @property
    def is_secure(self) -> bool:
        return self.url.startswith('https://')

    @property
    def has_chinese(self) -> bool:
        return bool(re.search(r'[\u4e00-\u9fff]', self.title))


@dataclass
class ClassificationResult:
    """分类结果"""
    category: str
    confidence: float
    subcategory: Optional[str] = None
    reasoning: List[str] = field(default_factory=list)
    alternatives: List[Tuple[str, float]] = field(default_factory=list)
    processing_time: float = 0.0
    method: str = "unknown"
    facets: Dict[str, str] = field(default_factory=dict)


class AIBookmarkClassifier:
    """AI智能书签分类器"""

    def __init__(self, config_path: str = "config.json", enable_ml: bool = True, config: Optional[Dict] = None):
        self.config_path = config_path
        self.enable_ml = enable_ml
        self.logger = logging.getLogger(__name__)

        # 延迟初始化组件
        self._config: Optional[Dict] = self._normalize_category_config(config) if isinstance(config, dict) else None
        self._rule_engine: Optional[RuleEngine] = None
        self._semantic_analyzer: Optional[SemanticAnalyzer] = None
        self._user_profiler: Optional[UserProfiler] = None
        self._performance_monitor: Optional[PerformanceMonitor] = None
        self._ml_classifier: Optional[MLClassifierWrapper] = None
        self._llm_classifier: Optional[LLMClassifier] = None

        # 缓存
        self.feature_cache: Dict[str, BookmarkFeatures] = {}
        self.classification_cache: Dict[str, ClassificationResult] = {}
        self._max_cache_size = 5000

        # 统计
        self.stats = {
            'total_classified': 0,
            'rule_engine': 0,
            'ml_classifier': 0,
            'semantic_analyzer': 0,
            'user_profiler': 0,
            'fallback': 0,
            'cache_hits': 0,
            'average_confidence': 0.0,
            'llm': 0,
        }

    @property
    def config(self) -> Dict:
        if self._config is None:
            self._config = self._load_config()
        return self._config

    @property
    def rule_engine(self) -> RuleEngine:
        if self._rule_engine is None:
            self._rule_engine = RuleEngine(self.config)
        return self._rule_engine

    @property
    def semantic_analyzer(self) -> SemanticAnalyzer:
        if self._semantic_analyzer is None:
            self._semantic_analyzer = SemanticAnalyzer()
        return self._semantic_analyzer

    @property
    def user_profiler(self) -> UserProfiler:
        if self._user_profiler is None:
            self._user_profiler = UserProfiler()
        return self._user_profiler

    @property
    def performance_monitor(self) -> PerformanceMonitor:
        if self._performance_monitor is None:
            self._performance_monitor = PerformanceMonitor()
        return self._performance_monitor

    @property
    def ml_classifier(self) -> Optional[MLClassifierWrapper]:
        if self._ml_classifier is None and self.enable_ml:
            try:
                self._ml_classifier = MLClassifierWrapper()
                self.logger.info("机器学习组件已启用")
            except Exception as e:
                self.logger.warning(f"机器学习组件初始化失败: {e}")
        return self._ml_classifier

    @property
    def llm_classifier(self) -> Optional[LLMClassifier]:
        if self._llm_classifier is None and LLMClassifier is not None:
            try:
                self._llm_classifier = LLMClassifier(self.config_path)
            except Exception as e:
                self.logger.warning(f"LLM 分类器初始化失败: {e}")
        return self._llm_classifier

    def _load_config(self) -> Dict:
        try:
            with open(self.config_path, 'r', encoding='utf-8') as f:
                config = json.load(f)
            
            # 加载智能规则并合并
            if SmartRuleLoader is not None and merge_with_main_config is not None:
                try:
                    loader = SmartRuleLoader()
                    smart_rules = loader.load_all()
                    config = merge_with_main_config(config, smart_rules)
                    self.logger.info(f"已加载智能规则: {smart_rules.get('_meta', {})}")
                except Exception as e:
                    self.logger.warning(f"智能规则加载失败，使用默认配置: {e}")
            
            return self._normalize_category_config(config)
        except Exception as e:
            self.logger.error(f"配置文件加载失败: {e}")
            return self._get_default_config()

    @staticmethod
    def _strip_category_prefix(text: str) -> str:
        if not text:
            return ""
        s = str(text).strip()
        i = 0
        while i < len(s) and not ("\u4e00" <= s[i] <= "\u9fff" or s[i].isalnum()):
            i += 1
        return s[i:].strip() if i < len(s) else s

    def _normalize_category_string(self, category: str) -> str:
        if not category:
            return ""
        cat = str(category).strip()
        if not cat:
            return ""
        if '/' in cat:
            main, sub = cat.split('/', 1)
            main_n = self._strip_category_prefix(main)
            sub_n = self._strip_category_prefix(sub)
            return f"{main_n}/{sub_n}" if sub_n else main_n
        return self._strip_category_prefix(cat)

    def _normalize_category_config(self, config: Dict) -> Dict:
        if not isinstance(config, dict):
            return {}

        normalized = dict(config)

        order = normalized.get('category_order')
        if isinstance(order, list):
            normalized['category_order'] = [self._strip_category_prefix(x) for x in order if str(x).strip()]

        dgr = normalized.get('domain_grouping_rules')
        if isinstance(dgr, dict):
            new_dgr = {}
            for k, v in dgr.items():
                nk = self._strip_category_prefix(k)
                if not nk:
                    continue
                if nk in new_dgr and isinstance(new_dgr[nk], list) and isinstance(v, list):
                    new_dgr[nk].extend(v)
                else:
                    new_dgr[nk] = v
            normalized['domain_grouping_rules'] = new_dgr

        pr = normalized.get('priority_rules')
        if isinstance(pr, dict):
            new_pr = {}
            for k, v in pr.items():
                nk = self._normalize_category_string(k)
                if not nk:
                    continue
                new_pr[nk] = v
            normalized['priority_rules'] = new_pr

        cr = normalized.get('category_rules')
        if isinstance(cr, dict):
            new_cr = {}
            for k, v in cr.items():
                nk = self._normalize_category_string(k)
                if not nk:
                    continue
                new_cr[nk] = v
            normalized['category_rules'] = new_cr

        return normalized

    def _get_default_config(self) -> Dict:
        return {
            "ai_settings": {
                "confidence_threshold": 0.7,
                "use_semantic_analysis": True,
                "use_user_profiling": True,
                "cache_size": 10000,
            },
            "category_rules": {
                "AI/机器学习": {
                    "rules": [
                        {"match": "domain", "keywords": ["openai.com", "huggingface.co"], "weight": 20},
                        {"match": "title", "keywords": ["machine learning", "深度学习", "neural", "AI"], "weight": 15},
                    ]
                },
                "技术/编程": {
                    "rules": [
                        {"match": "domain", "keywords": ["github.com", "stackoverflow.com"], "weight": 20},
                        {"match": "title", "keywords": ["programming", "code", "编程", "代码"], "weight": 10},
                    ]
                },
            },
            "category_hierarchy": {
                "AI": ["机器学习", "深度学习", "自然语言处理", "计算机视觉"],
                "技术": ["编程", "前端", "后端", "DevOps", "数据库"],
                "学习": ["教程", "文档", "课程", "书籍"],
                "工具": ["在线工具", "开发工具", "设计工具"],
            },
            "llm": {
                "enable": False,
                "provider": "openai",
                "base_url": "https://api.openai.com",
                "model": "gpt-4o-mini",
                "api_key_env": "OPENAI_API_KEY",
                "temperature": 0.0,
                "top_p": 1.0,
                "timeout_seconds": 25,
                "max_retries": 1,
                "organizer": {
                    "enable": False,
                    "max_examples_per_category": 5,
                    "max_domains_per_category": 5,
                    "max_tokens": 1800,
                    "force_json": True
                }
            },
        }

    def extract_features(self, url: str, title: str) -> BookmarkFeatures:
        cache_key = f"{url}::{title}"
        if cache_key in self.feature_cache:
            return self.feature_cache[cache_key]

        try:
            parsed = urlparse(url)
            domain = parsed.netloc.lower().replace('www.', '')
            path_segments = [seg for seg in parsed.path.split('/') if seg]

            # 解析查询参数
            query_params: Dict[str, str] = {}
            if parsed.query:
                for param in parsed.query.split('&'):
                    if '=' in param:
                        key, value = param.split('=', 1)
                        query_params[key] = value

            content_type = self._detect_content_type(url, title)
            language = self._detect_language(title)

            features = BookmarkFeatures(
                url=url,
                title=title,
                domain=domain,
                path_segments=path_segments,
                query_params=query_params,
                content_type=content_type,
                language=language,
            )

            if len(self.feature_cache) < self.config.get('ai_settings', {}).get('cache_size', 10000):
                self.feature_cache[cache_key] = features

            return features
        except Exception as e:
            self.logger.error(f"特征提取失败 {url}: {e}")
            return BookmarkFeatures(
                url=url, title=title, domain="", path_segments=[], query_params={}, content_type="unknown", language="unknown"
            )

    def classify(self, url: str, title: str) -> ClassificationResult:
        start_time = datetime.now()

        # 缓存命中
        cache_key = hashlib.md5(f"{url}::{title}".encode()).hexdigest()
        if cache_key in self.classification_cache:
            self.stats['cache_hits'] += 1
            cached = self.classification_cache[cache_key]
            cached.processing_time = (datetime.now() - start_time).total_seconds()
            return cached

        # 特征提取
        features = self.extract_features(url, title)

        # 多方法融合
        results: List[ClassificationResult] = []

        # 1) 规则引擎
        rule_result = self.rule_engine.classify(features)
        if rule_result:
            results.append(rule_result)

        # 2) 机器学习
        if self.ml_classifier:
            ml_result = self.ml_classifier.classify(features)
            if ml_result:
                results.append(ml_result)

        # 3) 语义分析
        if self.config.get('ai_settings', {}).get('use_semantic_analysis', True):
            semantic_result = self.semantic_analyzer.classify(features)
            if semantic_result:
                results.append(semantic_result)

        # 4) 用户画像
        if self.config.get('ai_settings', {}).get('use_user_profiling', True):
            user_result = self.user_profiler.classify(features)
            if user_result:
                results.append(user_result)

        # 5) LLM（可选）
        if self.llm_classifier and self.llm_classifier.enabled():
            try:
                llm_result = self.llm_classifier.classify(
                    url,
                    title,
                    context={
                        'domain': features.domain,
                        'content_type': features.content_type,
                        'language': features.language,
                    },
                )
                if llm_result:
                    results.append(llm_result)
            except Exception as e:
                self.logger.warning(f"LLM 分类调用失败: {e}")

        # 融合
        final_result = self._ensemble_classification(results, features)

        # 方法统计
        final_method = final_result.method
        if 'rule_engine' in final_method:
            self.stats['rule_engine'] += 1
        if 'machine_learning' in final_method:
            self.stats['ml_classifier'] += 1
        if 'semantic_analyzer' in final_method:
            self.stats['semantic_analyzer'] += 1
        if 'user_profiler' in final_method:
            self.stats['user_profiler'] += 1
        if 'llm' in final_method:
            self.stats['llm'] += 1
        if final_method == 'fallback':
            self.stats['fallback'] += 1

        # 时间统计
        final_result.processing_time = (datetime.now() - start_time).total_seconds()

        # 更新全局统计 & 缓存
        self._update_stats(final_result)
        self._cache_result(cache_key, final_result)
        return final_result

    def _cache_result(self, cache_key: str, result: ClassificationResult):
        if len(self.classification_cache) >= self._max_cache_size:
            oldest_key = next(iter(self.classification_cache))
            del self.classification_cache[oldest_key]
        self.classification_cache[cache_key] = result

    def _ensemble_classification(self, results: List[ClassificationResult], features: BookmarkFeatures) -> ClassificationResult:
        if not results:
            return ClassificationResult(
                category="未分类",
                confidence=0.0,
                reasoning=["没有找到合适的分类方法"],
                method="fallback",
            )

        # 加权投票
        category_scores = defaultdict(float)
        all_reasoning: List[str] = []
        methods_used: List[str] = []
        merged_facets: Dict[str, str] = {}

        method_weights = {
            'rule_engine': 0.50,  # 提高规则引擎权重
            'machine_learning': 0.15,  # 降低 ML 权重（因为模型可能过时）
            'semantic_analyzer': 0.10,
            'user_profiler': 0.10,
            'llm': 0.50,
        }

        for res in results:
            if isinstance(res, dict):
                method = res.get('method', 'unknown')
                category = self._normalize_category_string(res.get('category', '未分类')) or '未分类'
                confidence = res.get('confidence', 0.0)
                reasoning = res.get('reasoning', [])
                facets = res.get('facets', {}) or {}
            else:
                method = res.method
                category = self._normalize_category_string(res.category) or '未分类'
                confidence = res.confidence
                reasoning = res.reasoning
                facets = getattr(res, 'facets', {}) or {}

            weight = method_weights.get(method, 0.1)
            category_scores[category] += confidence * weight
            all_reasoning.extend(reasoning)
            methods_used.append(method)
            # 合并分面提示（保留先到先得，避免覆盖更强信号）
            for k, v in facets.items():
                if v and k not in merged_facets:
                    merged_facets[k] = v

        if not category_scores:
            return ClassificationResult(
                category="未分类",
                confidence=0.0,
                reasoning=["所有分类方法都失败"],
                method="error",
            )

        best_category = max(category_scores, key=category_scores.get)
        top_score = category_scores[best_category]
        total_score = sum(category_scores.values())
        confidence = top_score / total_score if total_score > 0 else 0.0

        alternatives = [
            (cat, score / total_score) for cat, score in category_scores.items() if cat != best_category and total_score > 0
        ]
        alternatives.sort(key=lambda x: x[1], reverse=True)

        subcategory = self._determine_subcategory(best_category, features)

        final_method = '+'.join(set(methods_used)) if methods_used else 'unknown'

        threshold = self.config.get('ai_settings', {}).get('confidence_threshold', 0.7)
        try:
            threshold = float(threshold)
        except Exception:
            threshold = 0.7
        if threshold < 0:
            threshold = 0.0
        if threshold > 1:
            threshold = 1.0

        if best_category != "未分类" and confidence < threshold:
            threshold_reasoning = list(all_reasoning)
            threshold_reasoning.append(
                f"最终置信度 {confidence:.2f} 低于阈值 {threshold:.2f}，标记为未分类"
            )

            threshold_alternatives = [(best_category, confidence)]
            for alt in alternatives:
                if alt[0] != best_category:
                    threshold_alternatives.append(alt)

            return ClassificationResult(
                category="未分类",
                subcategory=None,
                confidence=confidence,
                reasoning=threshold_reasoning,
                alternatives=threshold_alternatives[:3],
                method=final_method,
                facets=merged_facets,
            )

        return ClassificationResult(
            category=best_category,
            subcategory=subcategory,
            confidence=confidence,
            reasoning=all_reasoning,
            alternatives=alternatives[:3],
            method=final_method,
            facets=merged_facets,
        )

    def _determine_subcategory(self, category: str, features: BookmarkFeatures) -> Optional[str]:
        hierarchy = self.config.get('category_hierarchy', {})
        if category in hierarchy:
            subs = hierarchy[category]
            title_lower = features.title.lower()
            for sub in subs:
                if sub.lower() in title_lower:
                    return sub
        return None

    def _detect_content_type(self, url: str, title: str) -> str:
        url_lower = url.lower()
        title_lower = title.lower()

        if any(domain in url_lower for domain in ['youtube.com', 'bilibili.com', 'vimeo.com']):
            return 'video'
        if any(domain in url_lower for domain in ['github.com', 'gitlab.com']):
            return 'code_repository'
        if any(pattern in url_lower for pattern in ['docs.', 'documentation', 'wiki']):
            return 'documentation'
        if any(domain in url_lower for domain in ['arxiv.org', 'acm.org', 'ieee.org']):
            return 'academic_paper'
        if any(keyword in title_lower for keyword in ['news', '新闻', 'breaking']):
            return 'news'
        if any(keyword in title_lower for keyword in ['tool', '工具', 'online', 'generator']):
            return 'online_tool'
        return 'webpage'

    def _detect_language(self, title: str) -> str:
        if re.search(r'[\u4e00-\u9fff]', title):
            return 'zh'
        elif re.search(r'[a-zA-Z]', title):
            return 'en'
        else:
            return 'unknown'

    def _update_stats(self, result: ClassificationResult):
        self.stats['total_classified'] += 1
        total = self.stats['total_classified']
        old_avg = self.stats['average_confidence']
        self.stats['average_confidence'] = (old_avg * (total - 1) + result.confidence) / total

    def learn_from_feedback(self, url: str, title: str, correct_category: str, predicted_category: str):
        features = self.extract_features(url, title)
        self.user_profiler.update_preferences(features, correct_category)
        if self.ml_classifier:
            self.ml_classifier.online_learn(features, correct_category)
        cache_key = hashlib.md5(f"{url}::{title}".encode()).hexdigest()
        if cache_key in self.classification_cache:
            del self.classification_cache[cache_key]
        self.logger.debug(f"学习反馈: {predicted_category} -> {correct_category}")

    def get_statistics(self) -> Dict:
        total_predictions = self.stats['rule_engine'] + self.stats['ml_classifier'] + \
                            self.stats['semantic_analyzer'] + self.stats['user_profiler'] + \
                            self.stats['llm'] + self.stats['fallback']
        return {
            'total_classified': self.stats['total_classified'],
            'cache_hits': self.stats['cache_hits'],
            'cache_hit_rate': self.stats['cache_hits'] / max(self.stats['total_classified'], 1),
            'average_confidence': self.stats['average_confidence'],
            'classification_methods': {
                'rule_engine': self.stats['rule_engine'],
                'ml_classifier': self.stats['ml_classifier'],
                'semantic_analyzer': self.stats['semantic_analyzer'],
                'user_profiler': self.stats['user_profiler'],
                'llm': self.stats['llm'],
                'unclassified (fallback)': self.stats['fallback'],
                'total': total_predictions,
            },
            'ml_enabled': self.ml_classifier is not None,
        }

    def save_model(self, path: str = "models/ai_classifier.json"):
        os.makedirs(os.path.dirname(path), exist_ok=True)
        model_data = {
            'version': '2.0',
            'timestamp': datetime.now().isoformat(),
            'stats': self.stats,
            'user_profile': self.user_profiler.export_profile(),
            'config': self.config,
        }
        with open(path, 'w', encoding='utf-8') as f:
            json.dump(model_data, f, ensure_ascii=False, indent=2)
        if self.ml_classifier:
            self.ml_classifier.save_model()
        self.logger.info(f"模型已保存到: {path}")

    def load_model(self, path: str = "models/ai_classifier.json"):
        if not os.path.exists(path):
            self.logger.warning(f"模型文件不存在: {path}")
            return
        try:
            with open(path, 'r', encoding='utf-8') as f:
                model_data = json.load(f)
            self.stats = model_data.get('stats', self.stats)
            self.user_profiler.import_profile(model_data.get('user_profile', {}))
            if self.ml_classifier:
                self.ml_classifier.load_model()
            self.logger.info(f"模型已从 {path} 加载")
        except Exception as e:
            self.logger.error(f"模型加载失败: {e}")
